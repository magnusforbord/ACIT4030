{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMZQA7p1_VWf",
        "outputId": "b00bfc0c-3294-4812-a07a-edcb19e1d26a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/wheels\n",
            "‚úÖ Deps installed\n"
          ]
        }
      ],
      "source": [
        "# ===========================\n",
        "# 1) MOUNT DRIVE & INSTALL DEPS\n",
        "# ===========================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "DRIVE_WHEELS = \"/content/drive/MyDrive/wheels\"\n",
        "\n",
        "# Go to wheels dir and install\n",
        "import os\n",
        "assert os.path.isdir(DRIVE_WHEELS), f\"‚ùå Wheels folder not found at: {DRIVE_WHEELS}\"\n",
        "%cd $DRIVE_WHEELS\n",
        "\n",
        "!pip install -q portalocker-3.2.0-py3-none-any.whl\n",
        "!pip install -q iopath-0.1.10-py3-none-any.whl\n",
        "!pip install -q tqdm-4.67.1-py3-none-any.whl\n",
        "!pip install -q typing_extensions-4.15.0-py3-none-any.whl\n",
        "!pip install -q pytorch3d-0.7.8-cp312-cp312-linux_x86_64.whl\n",
        "\n",
        "# UI deps\n",
        "!pip install -q gradio pillow\n",
        "\n",
        "print(\"‚úÖ Deps installed\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# 2) CLONE REPO & SET PATH\n",
        "# ===========================\n",
        "import os, sys\n",
        "\n",
        "REPO_URL = \"https://github.com/jintn/acit4030-3d-project.git\"\n",
        "REPO_DIR = \"/content/acit4030-3d-project\"\n",
        "\n",
        "if not os.path.isdir(REPO_DIR):\n",
        "    %cd /content\n",
        "    !git clone {REPO_URL}\n",
        "\n",
        "if REPO_DIR not in sys.path:\n",
        "    sys.path.append(REPO_DIR)\n",
        "\n",
        "%cd $REPO_DIR\n",
        "assert os.path.exists(\"nerf_model.py\"), \"‚ùå nerf_model.py not found. Check repo folder.\"\n",
        "print(\"‚úÖ Repo ready:\", REPO_DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMzA8if2_d2O",
        "outputId": "e109f012-f9ab-48c7-dff3-7db40427ef12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/acit4030-3d-project\n",
            "‚úÖ Repo ready: /content/acit4030-3d-project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# 3) GLOBALS & HYPERPARAMS\n",
        "# ===========================\n",
        "# Paths\n",
        "DRIVE_EXPORT = \"/content/drive/MyDrive/nerf\"\n",
        "os.makedirs(DRIVE_EXPORT, exist_ok=True)\n",
        "MODEL_PATH  = f\"{DRIVE_EXPORT}/nerf_trained.pt\"\n",
        "CONFIG_PATH = f\"{DRIVE_EXPORT}/config.json\"\n",
        "\n",
        "# Training / rendering defaults\n",
        "NUM_VIEWS               = 40\n",
        "AZIMUTH_RANGE_DEG       = 180\n",
        "N_ITERS                 = 1000\n",
        "LR                      = 1e-3\n",
        "MC_RAYS_PER_IMAGE       = 1250\n",
        "PTS_PER_RAY             = 168\n",
        "VOLUME_EXTENT_WORLD     = 3.0\n",
        "RENDER_SCALE            = 2\n",
        "\n",
        "# Viewer cache resolution (speed vs. quality)\n",
        "AZ_STEP                 = 15\n",
        "EL_STEP                 = 10\n",
        "\n",
        "# Viewer defaults\n",
        "START_AZ                = 180\n",
        "START_EL                = 0\n",
        "\n",
        "import torch, json, numpy as np\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"‚úÖ Using device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VzhmU7iO_f9w",
        "outputId": "9c1e526c-a00f-46f5-fdd0-f9c1566c3551"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Using device: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# 4) IMPORTS (REPO + RENDERERS)\n",
        "# ===========================\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image, ImageFilter\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from nerf_model import NeuralRadianceField\n",
        "from utils.plot_image_grid import image_grid\n",
        "from utils.generate_cow_renders import generate_cow_renders\n",
        "from utils.helper_functions import (\n",
        "    generate_rotating_nerf,\n",
        "    huber,\n",
        "    show_full_render,\n",
        "    sample_images_at_mc_locs,\n",
        ")\n",
        "\n",
        "from pytorch3d.renderer import (\n",
        "    FoVPerspectiveCameras,\n",
        "    NDCMultinomialRaysampler,\n",
        "    MonteCarloRaysampler,\n",
        "    EmissionAbsorptionRaymarcher,\n",
        "    ImplicitRenderer,\n",
        "    look_at_view_transform,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Imports OK\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uS1PtKpP_iKI",
        "outputId": "d4454e47-1c4d-4b30-f77b-fe9b1982dd3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Imports OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------------------\n",
        "# Hash-based NeRF variant (Instant-NGP style multi-resolution hash encoding)\n",
        "# -------------------------------------------------------------------------\n",
        "import math\n",
        "from typing import Tuple\n",
        "\n",
        "from nerf_model import HarmonicEmbedding\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from pytorch3d.renderer import RayBundle, ray_bundle_to_ray_points\n",
        "\n",
        "\n",
        "class MultiResHashEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Simplified multi-resolution hash grid encoder inspired by Instant-NGP.\n",
        "\n",
        "    Takes 3D coordinates in [0, 1]^3 and returns a concatenated feature vector.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_levels: int = 8,\n",
        "        features_per_level: int = 2,\n",
        "        log2_hashmap_size: int = 15,\n",
        "        base_resolution: int = 16,\n",
        "        finest_resolution: int = 256,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_levels = num_levels\n",
        "        self.features_per_level = features_per_level\n",
        "        self.log2_hashmap_size = log2_hashmap_size\n",
        "\n",
        "        # Geometric progression of resolutions from base_resolution to finest_resolution\n",
        "        b = math.exp(\n",
        "            math.log(finest_resolution / base_resolution) / (num_levels - 1)\n",
        "        )\n",
        "\n",
        "        self.resolutions = []\n",
        "        self.embeddings = nn.ModuleList()\n",
        "\n",
        "        for level in range(num_levels):\n",
        "            res = int(base_resolution * (b ** level))\n",
        "            self.resolutions.append(res)\n",
        "\n",
        "            # Hash table size per level (capped by res^3)\n",
        "            hashmap_size = min(2 ** log2_hashmap_size, res ** 3)\n",
        "            emb = nn.Embedding(hashmap_size, features_per_level)\n",
        "            nn.init.uniform_(emb.weight, a=-1e-4, b=1e-4)\n",
        "            self.embeddings.append(emb)\n",
        "\n",
        "        # Fixed primes for simple 3D hash\n",
        "        self.register_buffer(\n",
        "            \"hash_primes\",\n",
        "            torch.tensor([1, 2654435761, 805459861], dtype=torch.long),\n",
        "        )\n",
        "\n",
        "    def hash_coords(self, coords: torch.LongTensor, level: int) -> torch.LongTensor:\n",
        "        \"\"\"\n",
        "        coords: (..., 3) integer grid coordinates\n",
        "        return: (...) hashed indices in [0, table_size)\n",
        "        \"\"\"\n",
        "        h = (coords * self.hash_primes).sum(dim=-1)\n",
        "        h = h & 0xFFFFFFFF  # 32-bit mask\n",
        "        table_size = self.embeddings[level].num_embeddings\n",
        "        return h % table_size\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        x: (..., 3) in [0, 1]^3\n",
        "        returns: (..., num_levels * features_per_level)\n",
        "        \"\"\"\n",
        "        assert x.shape[-1] == 3, \"Input to MultiResHashEncoder must be (..., 3).\"\n",
        "        orig_shape = x.shape[:-1]\n",
        "        x = x.view(-1, 3)  # (N, 3)\n",
        "\n",
        "        feats_per_level = []\n",
        "\n",
        "        for level, (res, emb) in enumerate(zip(self.resolutions, self.embeddings)):\n",
        "            # Scale to grid\n",
        "            pos = x * res  # in [0, res)\n",
        "            pos_floor = torch.floor(pos).long()\n",
        "            pos_frac = pos - pos_floor.float()\n",
        "\n",
        "            max_coord = res - 1\n",
        "            pos_floor = torch.clamp(pos_floor, 0, max_coord)\n",
        "\n",
        "            # 8 corner offsets for trilinear interpolation\n",
        "            offsets = torch.tensor(\n",
        "                [\n",
        "                    [0, 0, 0],\n",
        "                    [1, 0, 0],\n",
        "                    [0, 1, 0],\n",
        "                    [1, 1, 0],\n",
        "                    [0, 0, 1],\n",
        "                    [1, 0, 1],\n",
        "                    [0, 1, 1],\n",
        "                    [1, 1, 1],\n",
        "                ],\n",
        "                device=x.device,\n",
        "                dtype=torch.long,\n",
        "            )  # (8, 3)\n",
        "\n",
        "            # (N, 3) + (8, 3) -> (N, 8, 3)\n",
        "            corner_coords = pos_floor.unsqueeze(1) + offsets.unsqueeze(0)\n",
        "            corner_coords = torch.clamp(corner_coords, 0, max_coord)\n",
        "\n",
        "            # Flatten, hash, lookup\n",
        "            hashed = self.hash_coords(corner_coords.view(-1, 3), level)\n",
        "            corner_feats = emb(hashed)  # (N * 8, F)\n",
        "            corner_feats = corner_feats.view(-1, 8, self.features_per_level)\n",
        "\n",
        "            # Trilinear weights\n",
        "            fx, fy, fz = pos_frac.unbind(dim=-1)  # (N,)\n",
        "            fx = fx.view(-1, 1)\n",
        "            fy = fy.view(-1, 1)\n",
        "            fz = fz.view(-1, 1)\n",
        "\n",
        "            w000 = (1 - fx) * (1 - fy) * (1 - fz)\n",
        "            w100 = (fx) * (1 - fy) * (1 - fz)\n",
        "            w010 = (1 - fx) * (fy) * (1 - fz)\n",
        "            w110 = (fx) * (fy) * (1 - fz)\n",
        "            w001 = (1 - fx) * (1 - fy) * (fz)\n",
        "            w101 = (fx) * (1 - fy) * (fz)\n",
        "            w011 = (1 - fx) * (fy) * (fz)\n",
        "            w111 = (fx) * (fy) * (fz)\n",
        "\n",
        "            weights = torch.cat(\n",
        "                [w000, w100, w010, w110, w001, w101, w011, w111], dim=1\n",
        "            )  # (N, 8)\n",
        "\n",
        "            feat = (corner_feats * weights.unsqueeze(-1)).sum(dim=1)  # (N, F)\n",
        "            feats_per_level.append(feat)\n",
        "\n",
        "        encoded = torch.cat(feats_per_level, dim=-1)  # (N, L*F)\n",
        "        encoded = encoded.view(*orig_shape, self.num_levels * self.features_per_level)\n",
        "        return encoded\n",
        "\n",
        "\n",
        "class HashNeuralRadianceField(nn.Module):\n",
        "    \"\"\"\n",
        "    NeRF variant that:\n",
        "      * uses MultiResHashEncoder for 3D positions\n",
        "      * uses harmonic embedding for viewing directions\n",
        "    Interface matches the original NeuralRadianceField:\n",
        "      forward(ray_bundle: RayBundle) -> (densities, colors)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_levels: int = 8,\n",
        "        features_per_level: int = 2,\n",
        "        log2_hashmap_size: int = 15,\n",
        "        base_resolution: int = 16,\n",
        "        finest_resolution: int = 256,\n",
        "        n_hidden_neurons: int = 256,\n",
        "        n_harmonic_functions_dir: int = 60,\n",
        "        aabb_min: float = -1.0,\n",
        "        aabb_max: float = 1.0,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.aabb_min = aabb_min\n",
        "        self.aabb_max = aabb_max\n",
        "\n",
        "        # --- position encoder (hash grid) ---\n",
        "        self.pos_encoder = MultiResHashEncoder(\n",
        "            num_levels=num_levels,\n",
        "            features_per_level=features_per_level,\n",
        "            log2_hashmap_size=log2_hashmap_size,\n",
        "            base_resolution=base_resolution,\n",
        "            finest_resolution=finest_resolution,\n",
        "        )\n",
        "        pos_embedding_dim = num_levels * features_per_level\n",
        "\n",
        "        # --- direction encoder (reuse HarmonicEmbedding from this file) ---\n",
        "        self.dir_embedding = HarmonicEmbedding(n_harmonic_functions_dir)\n",
        "        dir_embedding_dim = n_harmonic_functions_dir * 2 * 3\n",
        "\n",
        "        # --- shared MLP for features ---\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(pos_embedding_dim, n_hidden_neurons),\n",
        "            nn.Softplus(beta=10.0),\n",
        "            nn.Linear(n_hidden_neurons, n_hidden_neurons),\n",
        "            nn.Softplus(beta=10.0),\n",
        "        )\n",
        "\n",
        "        # --- density branch ---\n",
        "        self.density_layer = nn.Sequential(\n",
        "            nn.Linear(n_hidden_neurons, 1),\n",
        "            nn.Softplus(beta=10.0),\n",
        "        )\n",
        "        # Initialize bias like original NeRF\n",
        "        self.density_layer[0].bias.data[0] = -1.5\n",
        "\n",
        "        # --- color branch ---\n",
        "        self.color_layer = nn.Sequential(\n",
        "            nn.Linear(n_hidden_neurons + dir_embedding_dim, n_hidden_neurons),\n",
        "            nn.Softplus(beta=10.0),\n",
        "            nn.Linear(n_hidden_neurons, 3),\n",
        "            nn.Sigmoid(),  # RGB in [0, 1]\n",
        "        )\n",
        "\n",
        "    def _normalize_points(self, points: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Map world coordinates from [aabb_min, aabb_max] to [0, 1] for encoding.\n",
        "        points: (..., 3)\n",
        "        \"\"\"\n",
        "        return (points - self.aabb_min) / (self.aabb_max - self.aabb_min + 1e-6)\n",
        "\n",
        "    def _get_densities(self, features: torch.Tensor) -> torch.Tensor:\n",
        "        raw_densities = self.density_layer(features)\n",
        "        return 1 - (-raw_densities).exp()  # same mapping as original NeRF\n",
        "\n",
        "    def _get_colors(self, features: torch.Tensor, rays_directions: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        View-dependent color, same idea as original NeRF: harmonic embedding of directions.\n",
        "        \"\"\"\n",
        "        spatial_size = features.shape[:-1]\n",
        "\n",
        "        # Normalize directions to unit length\n",
        "        rays_directions_normed = torch.nn.functional.normalize(\n",
        "            rays_directions, dim=-1\n",
        "        )\n",
        "        rays_embed = self.dir_embedding(rays_directions_normed)\n",
        "\n",
        "        # Expand to match feature spatial size\n",
        "        rays_embed_expand = rays_embed[..., None, :].expand(\n",
        "            *spatial_size, rays_embed.shape[-1]\n",
        "        )\n",
        "\n",
        "        color_input = torch.cat((features, rays_embed_expand), dim=-1)\n",
        "        return self.color_layer(color_input)\n",
        "\n",
        "    def forward(self, ray_bundle: RayBundle, **kwargs):\n",
        "        \"\"\"\n",
        "        Matches the original NeuralRadianceField.forward:\n",
        "          ray_bundle -> densities, colors\n",
        "        \"\"\"\n",
        "        # 1) Convert ray parametrization to 3D points in world coordinates\n",
        "        rays_points_world = ray_bundle_to_ray_points(ray_bundle)\n",
        "        # rays_points_world: [minibatch, ..., 3]\n",
        "\n",
        "        # 2) Normalize points for hash encoder\n",
        "        pos_norm = self._normalize_points(rays_points_world)\n",
        "\n",
        "        # 3) Hash encoding of positions\n",
        "        embeds = self.pos_encoder(pos_norm)\n",
        "\n",
        "        # 4) MLP to latent features\n",
        "        features = self.mlp(embeds)\n",
        "\n",
        "        # 5) Density & color heads\n",
        "        rays_densities = self._get_densities(features)\n",
        "        rays_colors = self._get_colors(features, ray_bundle.directions)\n",
        "\n",
        "        return rays_densities, rays_colors\n",
        "\n",
        "    def batched_forward(\n",
        "        self,\n",
        "        ray_bundle: RayBundle,\n",
        "        n_batches: int = 16,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Same pattern as the original NeuralRadianceField.batched_forward,\n",
        "        but calling this class's forward().\n",
        "        \"\"\"\n",
        "        n_pts_per_ray = ray_bundle.lengths.shape[-1]\n",
        "        spatial_size = [*ray_bundle.origins.shape[:-1], n_pts_per_ray]\n",
        "\n",
        "        tot_samples = ray_bundle.origins.shape[:-1].numel()\n",
        "        batches = torch.chunk(torch.arange(tot_samples, device=ray_bundle.origins.device), n_batches)\n",
        "\n",
        "        batch_outputs = [\n",
        "            self.forward(\n",
        "                RayBundle(\n",
        "                    origins=ray_bundle.origins.view(-1, 3)[batch_idx],\n",
        "                    directions=ray_bundle.directions.view(-1, 3)[batch_idx],\n",
        "                    lengths=ray_bundle.lengths.view(-1, n_pts_per_ray)[batch_idx],\n",
        "                    xys=None,\n",
        "                )\n",
        "            )\n",
        "            for batch_idx in batches\n",
        "        ]\n",
        "\n",
        "        rays_densities, rays_colors = [\n",
        "            torch.cat(\n",
        "                [batch_output[output_i] for batch_output in batch_outputs],\n",
        "                dim=0,\n",
        "            ).view(*spatial_size, -1)\n",
        "            for output_i in (0, 1)\n",
        "        ]\n",
        "\n",
        "        return rays_densities, rays_colors"
      ],
      "metadata": {
        "id": "5cI3uOwaCf42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# 5) SYNTHETIC DATA (COW RENDERS)\n",
        "# ===========================\n",
        "target_cameras, target_images, target_silhouettes = generate_cow_renders(\n",
        "    num_views=NUM_VIEWS, azimuth_range=AZIMUTH_RANGE_DEG\n",
        ")\n",
        "print(f\"Generated {len(target_images)} images/silhouettes/cameras.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpI98eDC_i0H",
        "outputId": "8a80a568-7697-460b-c1cf-907070392264"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated 40 images/silhouettes/cameras.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# 6) RENDERERS (MC SAMPLER + FULL GRID)\n",
        "# ===========================\n",
        "render_size = target_images.shape[1] * RENDER_SCALE\n",
        "\n",
        "raysampler_mc = MonteCarloRaysampler(\n",
        "    min_x=-1.0, max_x=1.0,\n",
        "    min_y=-1.0, max_y=1.0,\n",
        "    n_rays_per_image=MC_RAYS_PER_IMAGE,\n",
        "    n_pts_per_ray=PTS_PER_RAY,\n",
        "    min_depth=0.1, max_depth=VOLUME_EXTENT_WORLD,\n",
        ")\n",
        "renderer_mc = ImplicitRenderer(\n",
        "    raysampler=raysampler_mc, raymarcher=EmissionAbsorptionRaymarcher()\n",
        ")\n",
        "\n",
        "raysampler_grid = NDCMultinomialRaysampler(\n",
        "    image_height=render_size,\n",
        "    image_width=render_size,\n",
        "    n_pts_per_ray=PTS_PER_RAY,\n",
        "    min_depth=0.1,\n",
        "    max_depth=VOLUME_EXTENT_WORLD,\n",
        ")\n",
        "renderer_grid = ImplicitRenderer(\n",
        "    raysampler=raysampler_grid, raymarcher=EmissionAbsorptionRaymarcher()\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Renderers ready\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m55rohlw_kPk",
        "outputId": "1a2cf049-57dc-4c33-86ff-35cd5debb6cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Renderers ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# 7) MODEL ‚Üí DEVICE\n",
        "# ===========================\n",
        "torch.manual_seed(1)\n",
        "\n",
        "# Baseline:\n",
        "#neural_radiance_field = NeuralRadianceField()\n",
        "\n",
        "# Hash-based NeRF:\n",
        "neural_radiance_field = HashNeuralRadianceField(\n",
        "    num_levels=8,\n",
        "    features_per_level=2,\n",
        "    log2_hashmap_size=15,\n",
        "    base_resolution=16,\n",
        "    finest_resolution=256,\n",
        "    n_hidden_neurons=256,\n",
        "    n_harmonic_functions_dir=60,\n",
        "    aabb_min=-1.0,\n",
        "    aabb_max=1.0,\n",
        ")\n",
        "\n",
        "renderer_grid = renderer_grid.to(device)\n",
        "renderer_mc   = renderer_mc.to(device)\n",
        "target_cameras      = target_cameras.to(device)\n",
        "target_images       = target_images.to(device)\n",
        "target_silhouettes  = target_silhouettes.to(device)\n",
        "neural_radiance_field = neural_radiance_field.to(device)\n",
        "\n",
        "print(\"‚úÖ HashNeuralRadianceField & tensors on device\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfk8CDOe_ljq",
        "outputId": "071ab8c1-cc7f-4b01-eec0-e278197ef178"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ HashNeuralRadianceField & tensors on device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# 8) TRAIN LOOP\n",
        "# ===========================\n",
        "optimizer = torch.optim.Adam(neural_radiance_field.parameters(), lr=LR)\n",
        "batch_size = 6\n",
        "n_iter = N_ITERS\n",
        "\n",
        "loss_history_color, loss_history_sil = [], []\n",
        "\n",
        "for iteration in range(n_iter):\n",
        "    if iteration == round(n_iter * 0.75):\n",
        "        print(\"Decreasing LR 10-fold ...\")\n",
        "        optimizer = torch.optim.Adam(neural_radiance_field.parameters(), lr=LR * 0.1)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    batch_idx = torch.randperm(len(target_cameras))[:batch_size]\n",
        "\n",
        "    batch_cameras = FoVPerspectiveCameras(\n",
        "        R=target_cameras.R[batch_idx],\n",
        "        T=target_cameras.T[batch_idx],\n",
        "        znear=target_cameras.znear[batch_idx],\n",
        "        zfar=target_cameras.zfar[batch_idx],\n",
        "        aspect_ratio=target_cameras.aspect_ratio[batch_idx],\n",
        "        fov=target_cameras.fov[batch_idx],\n",
        "        device=device,\n",
        "    )\n",
        "\n",
        "    rendered_images_silhouettes, sampled_rays = renderer_mc(\n",
        "        cameras=batch_cameras,\n",
        "        volumetric_function=neural_radiance_field,\n",
        "    )\n",
        "    rendered_images, rendered_silhouettes = rendered_images_silhouettes.split([3, 1], dim=-1)\n",
        "\n",
        "    silhouettes_at_rays = sample_images_at_mc_locs(\n",
        "        target_silhouettes[batch_idx, ..., None], sampled_rays.xys\n",
        "    )\n",
        "    colors_at_rays = sample_images_at_mc_locs(\n",
        "        target_images[batch_idx], sampled_rays.xys\n",
        "    )\n",
        "\n",
        "    sil_err = huber(rendered_silhouettes, silhouettes_at_rays).abs().mean()\n",
        "    color_err = huber(rendered_images, colors_at_rays).abs().mean()\n",
        "    loss = color_err + sil_err\n",
        "\n",
        "    loss_history_color.append(float(color_err.detach()))\n",
        "    loss_history_sil.append(float(sil_err.detach()))\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if iteration % 100 == 0:\n",
        "        print(f\"Iteration {iteration}/{n_iter} | loss={loss.item():.4f}\")\n",
        "        show_idx = torch.randperm(len(target_cameras))[:1]\n",
        "        fig = show_full_render(\n",
        "            neural_radiance_field,\n",
        "            FoVPerspectiveCameras(\n",
        "                R=target_cameras.R[show_idx],\n",
        "                T=target_cameras.T[show_idx],\n",
        "                znear=target_cameras.znear[show_idx],\n",
        "                zfar=target_cameras.zfar[show_idx],\n",
        "                aspect_ratio=target_cameras.aspect_ratio[show_idx],\n",
        "                fov=target_cameras.fov[show_idx],\n",
        "                device=device,\n",
        "            ),\n",
        "            target_images[show_idx][0],\n",
        "            target_silhouettes[show_idx][0],\n",
        "            renderer_grid,\n",
        "            loss_history_color,\n",
        "            loss_history_sil,\n",
        "        )\n",
        "        fig.savefig(f\"intermediate_{iteration}.png\")\n",
        "        plt.close(fig)\n",
        "\n",
        "print(\"‚úÖ Training complete\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "2tY-EOZu_nzZ",
        "outputId": "51b3a403-d6d0-4b55-aed0-c2b171fb6cef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0/1000 | loss=0.3038\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 2.89 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.87 GiB is free. Process 49052 has 12.87 GiB memory in use. Of the allocated memory 9.93 GiB is allocated by PyTorch, and 2.63 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2305168915.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mloss_history_sil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msil_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m             )\n\u001b[0;32m--> 647\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    648\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    830\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.89 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.87 GiB is free. Process 49052 has 12.87 GiB memory in use. Of the allocated memory 9.93 GiB is allocated by PyTorch, and 2.63 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# 9) SAVE MODEL & CONFIG (VERSIONED)\n",
        "# ===========================\n",
        "import os, json, time, torch\n",
        "\n",
        "SAVE_DIR = \"/content/drive/MyDrive/nerf\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "# Create timestamped filenames so nothing is overwritten\n",
        "timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "MODEL_PATH_NEW  = f\"{SAVE_DIR}/nerf_hash_{timestamp}.pt\"\n",
        "CONFIG_PATH_NEW = f\"{SAVE_DIR}/config_hash_{timestamp}.json\"\n",
        "\n",
        "# --- save weights ---\n",
        "torch.save(neural_radiance_field.state_dict(), MODEL_PATH_NEW)\n",
        "print(f\"üíæ Saved BASELINE model to: {MODEL_PATH_NEW}\")\n",
        "\n",
        "# --- save config (training + rendering intrinsics) ---\n",
        "cfg = {\n",
        "    \"fov\": float(target_cameras.fov[0]),\n",
        "    \"aspect_ratio\": float(target_cameras.aspect_ratio[0]),\n",
        "    \"znear\": float(target_cameras.znear[0]),\n",
        "    \"zfar\": float(target_cameras.zfar[0]),\n",
        "    \"render_size\": int(target_images.shape[1] * RENDER_SCALE),\n",
        "    \"volume_extent_world\": float(VOLUME_EXTENT_WORLD),\n",
        "\n",
        "    # Also store training hyperparams (optional but useful)\n",
        "    \"training_iters\": N_ITERS,\n",
        "    \"learning_rate\": LR,\n",
        "    \"mc_rays_per_image\": MC_RAYS_PER_IMAGE,\n",
        "    \"pts_per_ray\": PTS_PER_RAY,\n",
        "}\n",
        "\n",
        "with open(CONFIG_PATH_NEW, \"w\") as f:\n",
        "    json.dump(cfg, f, indent=2)\n",
        "\n",
        "print(f\"üíæ Saved config to: {CONFIG_PATH_NEW}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sp_6DlhsCoPt",
        "outputId": "e0721deb-aa7e-4634-9c98-fb19a39f2752"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üíæ Saved BASELINE model to: /content/drive/MyDrive/nerf/nerf_hash_20251118_082228.pt\n",
            "üíæ Saved config to: /content/drive/MyDrive/nerf/config_hash_20251118_082228.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# 10b) LOAD HASH-NeRF MODEL & RENDERER FOR VIEWER\n",
        "# ===========================\n",
        "from pytorch3d.renderer import (\n",
        "    FoVPerspectiveCameras,\n",
        "    NDCMultinomialRaysampler,\n",
        "    EmissionAbsorptionRaymarcher,\n",
        "    ImplicitRenderer,\n",
        ")\n",
        "import json, torch, os\n",
        "\n",
        "# --- specify which hash model to load ---\n",
        "HASH_MODEL_PATH  = \"/content/drive/MyDrive/nerf/nerf_hash_20251118_082228.pt\"      # <-- INSERT YOUR CHECKPOINT\n",
        "HASH_CONFIG_PATH = \"/content/drive/MyDrive/nerf/config_hash_20251118_082228.json\"  # <-- INSERT MATCHING CONFIG\n",
        "\n",
        "assert os.path.exists(HASH_MODEL_PATH),  f\"‚ùå Model not found: {HASH_MODEL_PATH}\"\n",
        "assert os.path.exists(HASH_CONFIG_PATH), f\"‚ùå Config not found: {HASH_CONFIG_PATH}\"\n",
        "\n",
        "# --- load HashNeRF model ---\n",
        "model = HashNeuralRadianceField(\n",
        "    num_levels=8,\n",
        "    features_per_level=2,\n",
        "    log2_hashmap_size=15,\n",
        "    base_resolution=16,\n",
        "    finest_resolution=256,\n",
        "    n_hidden_neurons=256,\n",
        "    n_harmonic_functions_dir=60,\n",
        "    aabb_min=-1.0,\n",
        "    aabb_max=1.0,\n",
        ").to(device)\n",
        "\n",
        "state = torch.load(HASH_MODEL_PATH, map_location=device)\n",
        "model.load_state_dict(state)\n",
        "model.eval()\n",
        "print(\"‚úÖ Loaded HASH-NeRF model:\", HASH_MODEL_PATH)\n",
        "\n",
        "# --- load config ---\n",
        "with open(HASH_CONFIG_PATH, \"r\") as f:\n",
        "    cfg = json.load(f)\n",
        "print(\"‚úÖ Loaded HASH-NeRF config\")\n",
        "\n",
        "# --- rebuild renderer (same as baseline) ---\n",
        "raysampler_grid = NDCMultinomialRaysampler(\n",
        "    image_height=int(cfg[\"render_size\"]),\n",
        "    image_width=int(cfg[\"render_size\"]),\n",
        "    n_pts_per_ray=PTS_PER_RAY,\n",
        "    min_depth=0.1,\n",
        "    max_depth=float(cfg[\"volume_extent_world\"]),\n",
        ")\n",
        "renderer_grid = ImplicitRenderer(\n",
        "    raysampler=raysampler_grid,\n",
        "    raymarcher=EmissionAbsorptionRaymarcher(),\n",
        ").to(device)\n",
        "\n",
        "# --- default viewer camera ---\n",
        "base_cameras = FoVPerspectiveCameras(\n",
        "    device=device,\n",
        "    znear=torch.tensor([cfg[\"znear\"]], device=device),\n",
        "    zfar=torch.tensor([cfg[\"zfar\"]], device=device),\n",
        "    aspect_ratio=torch.tensor([cfg[\"aspect_ratio\"]], device=device),\n",
        "    fov=torch.tensor([cfg[\"fov\"]], device=device),\n",
        ")\n",
        "\n",
        "print(\"‚úÖ HASH-NeRF viewer renderer ready\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8D_-W5rYGzK",
        "outputId": "1a197ae1-07d6-4f51-9e7f-509a5448e384"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Loaded HASH-NeRF model: /content/drive/MyDrive/nerf/nerf_hash_20251118_082228.pt\n",
            "‚úÖ Loaded HASH-NeRF config\n",
            "‚úÖ HASH-NeRF viewer renderer ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# 11) GPU CACHE + INTERPOLATION + BLUR\n",
        "# ===========================\n",
        "from typing import Dict, Tuple\n",
        "\n",
        "CACHE_PATH = f\"{DRIVE_EXPORT}/views_AZ{AZ_STEP}_EL{EL_STEP}.npz\"\n",
        "\n",
        "_cache_tensor = None\n",
        "_key_to_idx: Dict[Tuple[int, int], int] = {}\n",
        "_H = _W = None\n",
        "\n",
        "def to_uint8_np_from_torch(img_t: torch.Tensor) -> np.ndarray:\n",
        "    return (img_t.clamp(0,1).detach().cpu().numpy() * 255).astype(np.uint8)\n",
        "\n",
        "@torch.no_grad()\n",
        "def render_full_gpu(elev_deg: float, azim_deg: float, dist: float = 2.7) -> torch.Tensor:\n",
        "    R, T = look_at_view_transform(dist=dist, elev=elev_deg, azim=azim_deg, device=device)\n",
        "    cam = FoVPerspectiveCameras(\n",
        "        R=R, T=T,\n",
        "        znear=base_cameras.znear, zfar=base_cameras.zfar,\n",
        "        aspect_ratio=base_cameras.aspect_ratio, fov=base_cameras.fov,\n",
        "        device=device,\n",
        "    )\n",
        "    img_sil, _ = renderer_grid(cameras=cam, volumetric_function=model.batched_forward)\n",
        "    return img_sil[..., :3].squeeze(0)\n",
        "\n",
        "@torch.no_grad()\n",
        "def precompute_cache_gpu():\n",
        "    global _cache_tensor, _key_to_idx, _H, _W\n",
        "    azs = np.arange(0, 361, AZ_STEP)\n",
        "    els = np.arange(-30, 31, EL_STEP)\n",
        "    imgs = []\n",
        "    _key_to_idx.clear()\n",
        "    idx = 0\n",
        "    for el in els:\n",
        "        for az in azs:\n",
        "            k = (int(az) % 360, int(np.clip(el, -30, 30)))\n",
        "            if k not in _key_to_idx:\n",
        "                img = render_full_gpu(el, az)\n",
        "                imgs.append(img)\n",
        "                _key_to_idx[k] = idx\n",
        "                idx += 1\n",
        "    _cache_tensor = torch.stack(imgs, dim=0)\n",
        "    _H, _W = _cache_tensor.shape[1:3]\n",
        "    print(f\"‚ö° Cached {len(_key_to_idx)} views on {device} (AZ {AZ_STEP}¬∞, EL {EL_STEP}¬∞).\")\n",
        "\n",
        "def save_cache_npz_gpu(path: str):\n",
        "    if _cache_tensor is None:\n",
        "        return\n",
        "    imgs_u8 = (_cache_tensor.clamp(0,1).mul(255).byte().cpu().numpy())\n",
        "    keys = np.array(list(_key_to_idx.keys()), dtype=object)\n",
        "    order = np.array([_key_to_idx[k] for k in _key_to_idx], dtype=np.int32)\n",
        "    sort_idx = np.argsort(order)\n",
        "    np.savez_compressed(path, keys=keys[sort_idx], imgs=imgs_u8[sort_idx])\n",
        "    print(f\"üíæ Saved GPU cache ‚Üí {path} ({len(keys)} views)\")\n",
        "\n",
        "def load_cache_npz_gpu(path: str) -> bool:\n",
        "    global _cache_tensor, _key_to_idx, _H, _W\n",
        "    if not os.path.exists(path):\n",
        "        return False\n",
        "    data = np.load(path, allow_pickle=True)\n",
        "    keys = list(data[\"keys\"])\n",
        "    imgs = data[\"imgs\"]\n",
        "    _key_to_idx.clear()\n",
        "    for i, k in enumerate(keys):\n",
        "        _key_to_idx[tuple(k)] = i\n",
        "    _cache_tensor = torch.from_numpy(imgs.astype(np.float32) / 255.0).to(device)\n",
        "    _H, _W = _cache_tensor.shape[1:3]\n",
        "    print(f\"üì• Loaded GPU cache from {path} ({len(_key_to_idx)} views).\")\n",
        "    return True\n",
        "\n",
        "def _get_four_indices(az, el):\n",
        "    az0 = int(np.floor(az / AZ_STEP) * AZ_STEP) % 360\n",
        "    az1 = (az0 + AZ_STEP) % 360\n",
        "    el0 = int(np.clip(np.floor((el + 30) / EL_STEP) * EL_STEP - 30, -30, 30))\n",
        "    el1 = int(np.clip(el0 + EL_STEP, -30, 30))\n",
        "    return (az0, el0), (az1, el0), (az0, el1), (az1, el1)\n",
        "\n",
        "@torch.no_grad()\n",
        "def bilinear_preview_gpu(az, el) -> torch.Tensor:\n",
        "    (az0, el0), (az1, el0b), (az0b, el1), (az1b, el1b) = _get_four_indices(az, el)\n",
        "    i00 = _key_to_idx[(az0, el0)]\n",
        "    i10 = _key_to_idx[(az1, el0b)]\n",
        "    i01 = _key_to_idx[(az0b, el1)]\n",
        "    i11 = _key_to_idx[(az1b, el1b)]\n",
        "    I00 = _cache_tensor[i00]\n",
        "    I10 = _cache_tensor[i10]\n",
        "    I01 = _cache_tensor[i01]\n",
        "    I11 = _cache_tensor[i11]\n",
        "    t = torch.tensor(((az - az0) % 360) / AZ_STEP, device=device).float()\n",
        "    u = torch.tensor((el - el0) / max(EL_STEP, 1e-6), device=device).float()\n",
        "    top = (1 - t) * I00 + t * I10\n",
        "    bot = (1 - t) * I01 + t * I11\n",
        "    return (1 - u) * top + u * bot\n",
        "\n",
        "def gaussian_kernel1d(radius: int, sigma: float, device):\n",
        "    x = torch.arange(-radius, radius+1, device=device)\n",
        "    w = torch.exp(-(x**2)/(2*sigma*sigma))\n",
        "    w = w / w.sum()\n",
        "    return w\n",
        "\n",
        "@torch.no_grad()\n",
        "def blur_preview_gpu(img: torch.Tensor, radius: int = 2, sigma: float = 1.5) -> torch.Tensor:\n",
        "    k1d = gaussian_kernel1d(radius, sigma, device=img.device)\n",
        "    x = img.permute(2,0,1).unsqueeze(0)\n",
        "    kh = k1d.view(1,1,1,-1)\n",
        "    x = F.conv2d(x, kh.expand(3,1,1,kh.shape[-1]), padding=(0, radius), groups=3)\n",
        "    kv = k1d.view(1,1,-1,1)\n",
        "    x = F.conv2d(x, kv.expand(3,1,kv.shape[-2],1), padding=(radius, 0), groups=3)\n",
        "    return x.squeeze(0).permute(1,2,0)"
      ],
      "metadata": {
        "id": "BeYH50iV_rjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- Helpers to pick the latest cache automatically ----\n",
        "import io, hashlib, glob, os, re, time\n",
        "\n",
        "def model_md5(model) -> str:\n",
        "    buf = io.BytesIO()\n",
        "    torch.save(model.state_dict(), buf)\n",
        "    return hashlib.md5(buf.getvalue()).hexdigest()[:8]\n",
        "\n",
        "def find_latest_cache(cache_dir: str, az_step: int, el_step: int, preferred_hash: str | None):\n",
        "    \"\"\"\n",
        "    Return (path, reason) where `path` is the best cache to use:\n",
        "      1) exact match for AZ/EL and preferred_hash, newest mtime if multiple\n",
        "      2) else newest file that matches AZ/EL regardless of hash\n",
        "      3) else None\n",
        "    Expected filename pattern: views_AZ{az}_EL{el}_{hash}.npz\n",
        "    \"\"\"\n",
        "    pattern = os.path.join(cache_dir, f\"views_AZ{az_step}_EL{el_step}_*.npz\")\n",
        "    candidates = glob.glob(pattern)\n",
        "    if not candidates:\n",
        "        return None, \"no cache files found\"\n",
        "\n",
        "    # extract (mtime, path, hash)\n",
        "    rx = re.compile(rf\"views_AZ{az_step}_EL{el_step}_(?P<h>[0-9a-fA-F]+)\\.npz$\")\n",
        "    parsed = []\n",
        "    for p in candidates:\n",
        "        m = rx.search(os.path.basename(p))\n",
        "        h = m.group(\"h\") if m else None\n",
        "        parsed.append((os.path.getmtime(p), p, h))\n",
        "\n",
        "    # 1) prefer exact hash match if available\n",
        "    if preferred_hash:\n",
        "        exact = [t for t in parsed if t[2] == preferred_hash]\n",
        "        if exact:\n",
        "            exact.sort(key=lambda t: t[0], reverse=True)\n",
        "            return exact[0][1], \"match model hash\"\n",
        "\n",
        "    # 2) otherwise newest by mtime\n",
        "    parsed.sort(key=lambda t: t[0], reverse=True)\n",
        "    return parsed[0][1], \"newest by mtime\""
      ],
      "metadata": {
        "id": "fQF8FV0cC--A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# 12) BUILD/LOAD CACHE + CALLBACKS (auto-pick latest)\n",
        "# ===========================\n",
        "FORCE_REBUILD_CACHE = False  # set True to force regeneration\n",
        "\n",
        "# derive preferred cache name for *this* model\n",
        "PREFERRED_HASH = model_md5(model)\n",
        "CACHE_DIR = DRIVE_EXPORT\n",
        "\n",
        "# Optionally print which cache we intend to use\n",
        "print(f\"‚ÑπÔ∏è Preferred cache hash for this model: {PREFERRED_HASH}\")\n",
        "\n",
        "def build_and_save_cache():\n",
        "    global _cache_tensor, _key_to_idx, _H, _W\n",
        "    _cache_tensor = None\n",
        "    _key_to_idx.clear()\n",
        "    torch.cuda.empty_cache()\n",
        "    precompute_cache_gpu()\n",
        "    # name includes steps + model hash so caches don‚Äôt get mixed up\n",
        "    out_path = os.path.join(CACHE_DIR, f\"views_AZ{AZ_STEP}_EL{EL_STEP}_{PREFERRED_HASH}.npz\")\n",
        "    save_cache_npz_gpu(out_path)\n",
        "    return out_path\n",
        "\n",
        "if FORCE_REBUILD_CACHE:\n",
        "    print(\"üîÅ FORCE_REBUILD_CACHE=True ‚Üí rebuilding cache now‚Ä¶\")\n",
        "    CACHE_PATH = build_and_save_cache()\n",
        "else:\n",
        "    # try to find the best existing cache\n",
        "    CACHE_PATH, reason = find_latest_cache(CACHE_DIR, AZ_STEP, EL_STEP, preferred_hash=PREFERRED_HASH)\n",
        "    if CACHE_PATH is not None and load_cache_npz_gpu(CACHE_PATH):\n",
        "        print(f\"‚úÖ Loaded cache: {CACHE_PATH} ({reason})\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è No suitable cache found ‚Üí building a new one‚Ä¶\")\n",
        "        CACHE_PATH = build_and_save_cache()\n",
        "\n",
        "@torch.no_grad()\n",
        "def on_change(azim, elev, mode, current_img):\n",
        "    img = bilinear_preview_gpu(azim, elev)\n",
        "    if mode == \"Blur preview\" and img is not None:\n",
        "        img = blur_preview_gpu(img, radius=2, sigma=1.5)\n",
        "    return to_uint8_np_from_torch(img)\n",
        "\n",
        "@torch.no_grad()\n",
        "def on_release(azim, elev):\n",
        "    img = render_full_gpu(elev, azim)\n",
        "    return to_uint8_np_from_torch(img)\n",
        "\n",
        "START_IMG_T = render_full_gpu(START_EL, START_AZ)\n",
        "START_IMG = to_uint8_np_from_torch(START_IMG_T)\n",
        "H, W = START_IMG.shape[:2]\n",
        "print(\"‚úÖ Viewer callbacks ready\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PliIgLpR_wbf",
        "outputId": "fae7e598-78ec-4a19-cd23-173f31a39075"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ÑπÔ∏è Preferred cache hash for this model: e12ee8e9\n",
            "üì• Loaded GPU cache from /content/drive/MyDrive/nerf/views_AZ15_EL10_65f687b2.npz (168 views).\n",
            "‚úÖ Loaded cache: /content/drive/MyDrive/nerf/views_AZ15_EL10_65f687b2.npz (newest by mtime)\n",
            "‚úÖ Viewer callbacks ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# 13) GRADIO UI\n",
        "# ===========================\n",
        "import gradio as gr\n",
        "\n",
        "with gr.Blocks(title=\"NeRF Viewer\") as demo:\n",
        "    gr.Markdown(\"## üêÑ NeRF Interactive Viewer\\nDrag on the image to rotate. Release to render full quality.\")\n",
        "    with gr.Row():\n",
        "        image_out = gr.Image(\n",
        "            value=START_IMG, label=\"Render\",\n",
        "            type=\"numpy\", height=H, width=W, interactive=True\n",
        "        )\n",
        "        with gr.Column(scale=0):\n",
        "            az = gr.Slider(0, 360, value=START_AZ, step=1, label=\"Azimuth (¬∞)\")\n",
        "            el = gr.Slider(-30, 30, value=START_EL, step=1, label=\"Elevation (¬∞)\")\n",
        "            quality = gr.Radio(\n",
        "                choices=[\"Bilinear preview\", \"Blur preview\"],\n",
        "                value=\"Bilinear preview\",\n",
        "                label=\"Drag preview\"\n",
        "            )\n",
        "\n",
        "    # Client-side JS: drag the image to update sliders (which triggers previews)\n",
        "    drag_bind = gr.HTML(\"\"\"\n",
        "    <script>\n",
        "    (function(){\n",
        "      const sleep = (ms) => new Promise(r => setTimeout(r, ms));\n",
        "      async function bind() {\n",
        "        for (let i=0;i<50;i++){\n",
        "          const app = window.gradioApp?.();\n",
        "          if (app) break;\n",
        "          await sleep(100);\n",
        "        }\n",
        "        const app = window.gradioApp?.();\n",
        "        if (!app) return;\n",
        "        const img = app.querySelector('div.svelte-1ipelgc img, .image-container img');\n",
        "        const az = app.querySelector('input[type=\"range\"][min=\"0\"][max=\"360\"]');\n",
        "        const el = app.querySelector('input[type=\"range\"][min=\"-30\"][max=\"30\"]');\n",
        "        if (!img || !az || !el) return;\n",
        "\n",
        "        let dragging = false, lastX = 0, lastY = 0;\n",
        "        const clamp = (v,min,max)=>Math.max(min,Math.min(max,v));\n",
        "        const step = (v,s)=>Math.round(v/s)*s;\n",
        "\n",
        "        img.addEventListener('mousedown', (e)=>{\n",
        "          dragging = true; lastX = e.clientX; lastY = e.clientY; e.preventDefault();\n",
        "        });\n",
        "        window.addEventListener('mouseup', ()=>{ dragging=false; });\n",
        "        window.addEventListener('mousemove', (e)=>{\n",
        "          if(!dragging) return;\n",
        "          const dx = e.clientX - lastX;\n",
        "          const dy = e.clientY - lastY;\n",
        "          lastX = e.clientX; lastY = e.clientY;\n",
        "\n",
        "          const AZ_SENS = 0.5;\n",
        "          const EL_SENS = 0.3;\n",
        "\n",
        "          let azVal = (parseFloat(az.value) + dx * AZ_SENS) % 360;\n",
        "          if(azVal < 0) azVal += 360;\n",
        "          let elVal = clamp(parseFloat(el.value) - dy * EL_SENS, -30, 30);\n",
        "\n",
        "          az.value = String(step(azVal,1));\n",
        "          el.value = String(step(elVal,1));\n",
        "\n",
        "          az.dispatchEvent(new Event('input', {bubbles:true}));\n",
        "          el.dispatchEvent(new Event('input', {bubbles:true}));\n",
        "        });\n",
        "        window.addEventListener('mouseup', ()=>{\n",
        "          az.dispatchEvent(new Event('change', {bubbles:true}));\n",
        "          el.dispatchEvent(new Event('change', {bubbles:true}));\n",
        "        });\n",
        "      }\n",
        "      bind();\n",
        "    })();\n",
        "    </script>\n",
        "    \"\"\")\n",
        "\n",
        "    az.change(on_change, inputs=[az, el, quality, image_out], outputs=image_out, queue=False)\n",
        "    el.change(on_change, inputs=[az, el, quality, image_out], outputs=image_out, queue=False)\n",
        "    az.release(on_release, inputs=[az, el], outputs=image_out, queue=True)\n",
        "    el.release(on_release, inputs=[az, el], outputs=image_out, queue=True)\n",
        "\n",
        "demo.launch(inline=True, debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "id": "5mQ0H52y_xZB",
        "outputId": "b9e6f760-34a0-4ca8-a80d-61889e5752ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://f42337b1e819e70804.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://f42337b1e819e70804.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://f42337b1e819e70804.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ]
}