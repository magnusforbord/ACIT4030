{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# 1) MOUNT DRIVE & INSTALL DEPS\n",
        "# ===========================\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "DRIVE_WHEELS = \"/content/drive/MyDrive/wheels\"\n",
        "\n",
        "# Go to wheels dir and install\n",
        "import os\n",
        "assert os.path.isdir(DRIVE_WHEELS), f\"‚ùå Wheels folder not found at: {DRIVE_WHEELS}\"\n",
        "%cd $DRIVE_WHEELS\n",
        "\n",
        "!pip install -q portalocker-3.2.0-py3-none-any.whl\n",
        "!pip install -q iopath-0.1.10-py3-none-any.whl\n",
        "!pip install -q tqdm-4.67.1-py3-none-any.whl\n",
        "!pip install -q typing_extensions-4.15.0-py3-none-any.whl\n",
        "!pip install -q pytorch3d-0.7.8-cp312-cp312-linux_x86_64.whl\n",
        "\n",
        "# UI deps\n",
        "!pip install -q gradio pillow\n",
        "\n",
        "print(\"‚úÖ Deps installed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxYHRKV80yXY",
        "outputId": "51bbf3f0-b6c8-4a5a-f7d3-184ff590977d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/wheels\n",
            "‚úÖ Deps installed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# 2) CLONE REPO & SET PATH\n",
        "# ===========================\n",
        "import os, sys\n",
        "\n",
        "REPO_URL = \"https://github.com/jintn/acit4030-3d-project.git\"\n",
        "REPO_DIR = \"/content/acit4030-3d-project\"\n",
        "\n",
        "if not os.path.isdir(REPO_DIR):\n",
        "    %cd /content\n",
        "    !git clone {REPO_URL}\n",
        "\n",
        "if REPO_DIR not in sys.path:\n",
        "    sys.path.append(REPO_DIR)\n",
        "\n",
        "%cd $REPO_DIR\n",
        "assert os.path.exists(\"nerf_model.py\"), \"‚ùå nerf_model.py not found. Check repo folder.\"\n",
        "print(\"‚úÖ Repo ready:\", REPO_DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtlPI3ec1O1l",
        "outputId": "8dab5717-0efd-491b-c2d6-2ac3ac1f6e44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/acit4030-3d-project\n",
            "‚úÖ Repo ready: /content/acit4030-3d-project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# 3) GLOBALS & HYPERPARAMS\n",
        "# ===========================\n",
        "# Paths\n",
        "DRIVE_EXPORT = \"/content/drive/MyDrive/nerf\"\n",
        "os.makedirs(DRIVE_EXPORT, exist_ok=True)\n",
        "MODEL_PATH  = f\"{DRIVE_EXPORT}/nerf_trained.pt\"\n",
        "CONFIG_PATH = f\"{DRIVE_EXPORT}/config.json\"\n",
        "\n",
        "# Training / rendering defaults\n",
        "NUM_VIEWS               = 50\n",
        "AZIMUTH_RANGE_DEG       = 180\n",
        "N_ITERS                 = 2000\n",
        "LR                      = 1e-3\n",
        "MC_RAYS_PER_IMAGE       = 1000\n",
        "PTS_PER_RAY             = 168\n",
        "VOLUME_EXTENT_WORLD     = 3.0\n",
        "RENDER_SCALE            = 2\n",
        "\n",
        "# Viewer cache resolution (speed vs. quality)\n",
        "AZ_STEP                 = 15\n",
        "EL_STEP                 = 10\n",
        "\n",
        "# Viewer defaults\n",
        "START_AZ                = 180\n",
        "START_EL                = 0\n",
        "\n",
        "import torch, json, numpy as np\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"‚úÖ Using device:\", device)"
      ],
      "metadata": {
        "id": "FJR6OhJEXuaL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a44b84f-f092-477c-e398-2a5e1d35e15d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Using device: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# 4) IMPORTS (REPO + RENDERERS)\n",
        "# ===========================\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image, ImageFilter\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from nerf_model import NeuralRadianceField\n",
        "from utils.plot_image_grid import image_grid\n",
        "from utils.generate_cow_renders import generate_cow_renders\n",
        "from utils.helper_functions import (\n",
        "    generate_rotating_nerf,\n",
        "    huber,\n",
        "    show_full_render,\n",
        "    sample_images_at_mc_locs,\n",
        ")\n",
        "\n",
        "from pytorch3d.renderer import (\n",
        "    FoVPerspectiveCameras,\n",
        "    NDCMultinomialRaysampler,\n",
        "    MonteCarloRaysampler,\n",
        "    EmissionAbsorptionRaymarcher,\n",
        "    ImplicitRenderer,\n",
        "    look_at_view_transform,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Imports OK\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKrKPGmsWn3S",
        "outputId": "e05d66de-c20f-4191-9121-4bce4a6123a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Imports OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# 5) SYNTHETIC DATA (COW RENDERS)\n",
        "# ===========================\n",
        "target_cameras, target_images, target_silhouettes = generate_cow_renders(\n",
        "    num_views=NUM_VIEWS, azimuth_range=AZIMUTH_RANGE_DEG\n",
        ")\n",
        "print(f\"Generated {len(target_images)} images/silhouettes/cameras.\")"
      ],
      "metadata": {
        "id": "DFZECRV0cGEW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67091ce4-e625-4b55-e205-5579f57903ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated 50 images/silhouettes/cameras.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# 6) RENDERERS (MC SAMPLER + FULL GRID)\n",
        "# ===========================\n",
        "render_size = target_images.shape[1] * RENDER_SCALE\n",
        "\n",
        "raysampler_mc = MonteCarloRaysampler(\n",
        "    min_x=-1.0, max_x=1.0,\n",
        "    min_y=-1.0, max_y=1.0,\n",
        "    n_rays_per_image=MC_RAYS_PER_IMAGE,\n",
        "    n_pts_per_ray=PTS_PER_RAY,\n",
        "    min_depth=0.1, max_depth=VOLUME_EXTENT_WORLD,\n",
        ")\n",
        "renderer_mc = ImplicitRenderer(\n",
        "    raysampler=raysampler_mc, raymarcher=EmissionAbsorptionRaymarcher()\n",
        ")\n",
        "\n",
        "raysampler_grid = NDCMultinomialRaysampler(\n",
        "    image_height=render_size,\n",
        "    image_width=render_size,\n",
        "    n_pts_per_ray=PTS_PER_RAY,\n",
        "    min_depth=0.1,\n",
        "    max_depth=VOLUME_EXTENT_WORLD,\n",
        ")\n",
        "renderer_grid = ImplicitRenderer(\n",
        "    raysampler=raysampler_grid, raymarcher=EmissionAbsorptionRaymarcher()\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Renderers ready\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtjAT2upWqYk",
        "outputId": "d6994876-2179-4441-f0f2-61c3a7419883"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Renderers ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# 7) MODEL ‚Üí DEVICE\n",
        "# ===========================\n",
        "torch.manual_seed(1)\n",
        "neural_radiance_field = NeuralRadianceField()\n",
        "\n",
        "renderer_grid = renderer_grid.to(device)\n",
        "renderer_mc   = renderer_mc.to(device)\n",
        "target_cameras      = target_cameras.to(device)\n",
        "target_images       = target_images.to(device)\n",
        "target_silhouettes  = target_silhouettes.to(device)\n",
        "neural_radiance_field = neural_radiance_field.to(device)\n",
        "\n",
        "print(\"‚úÖ Model & tensors on device\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KH5xwuRIWq-p",
        "outputId": "a510aa93-e6a8-4db3-face-d2961e4e58d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Model & tensors on device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# 8) TRAIN LOOP\n",
        "# ===========================\n",
        "optimizer = torch.optim.Adam(neural_radiance_field.parameters(), lr=LR)\n",
        "batch_size = 6\n",
        "n_iter = N_ITERS\n",
        "\n",
        "loss_history_color, loss_history_sil = [], []\n",
        "\n",
        "for iteration in range(n_iter):\n",
        "    if iteration == round(n_iter * 0.75):\n",
        "        print(\"Decreasing LR 10-fold ...\")\n",
        "        optimizer = torch.optim.Adam(neural_radiance_field.parameters(), lr=LR * 0.1)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    batch_idx = torch.randperm(len(target_cameras))[:batch_size]\n",
        "\n",
        "    batch_cameras = FoVPerspectiveCameras(\n",
        "        R=target_cameras.R[batch_idx],\n",
        "        T=target_cameras.T[batch_idx],\n",
        "        znear=target_cameras.znear[batch_idx],\n",
        "        zfar=target_cameras.zfar[batch_idx],\n",
        "        aspect_ratio=target_cameras.aspect_ratio[batch_idx],\n",
        "        fov=target_cameras.fov[batch_idx],\n",
        "        device=device,\n",
        "    )\n",
        "\n",
        "    rendered_images_silhouettes, sampled_rays = renderer_mc(\n",
        "        cameras=batch_cameras,\n",
        "        volumetric_function=neural_radiance_field,\n",
        "    )\n",
        "    rendered_images, rendered_silhouettes = rendered_images_silhouettes.split([3, 1], dim=-1)\n",
        "\n",
        "    silhouettes_at_rays = sample_images_at_mc_locs(\n",
        "        target_silhouettes[batch_idx, ..., None], sampled_rays.xys\n",
        "    )\n",
        "    colors_at_rays = sample_images_at_mc_locs(\n",
        "        target_images[batch_idx], sampled_rays.xys\n",
        "    )\n",
        "\n",
        "    sil_err = huber(rendered_silhouettes, silhouettes_at_rays).abs().mean()\n",
        "    color_err = huber(rendered_images, colors_at_rays).abs().mean()\n",
        "    loss = color_err + sil_err\n",
        "\n",
        "    loss_history_color.append(float(color_err.detach()))\n",
        "    loss_history_sil.append(float(sil_err.detach()))\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if iteration % 100 == 0:\n",
        "        print(f\"Iteration {iteration}/{n_iter} | loss={loss.item():.4f}\")\n",
        "        show_idx = torch.randperm(len(target_cameras))[:1]\n",
        "        fig = show_full_render(\n",
        "            neural_radiance_field,\n",
        "            FoVPerspectiveCameras(\n",
        "                R=target_cameras.R[show_idx],\n",
        "                T=target_cameras.T[show_idx],\n",
        "                znear=target_cameras.znear[show_idx],\n",
        "                zfar=target_cameras.zfar[show_idx],\n",
        "                aspect_ratio=target_cameras.aspect_ratio[show_idx],\n",
        "                fov=target_cameras.fov[show_idx],\n",
        "                device=device,\n",
        "            ),\n",
        "            target_images[show_idx][0],\n",
        "            target_silhouettes[show_idx][0],\n",
        "            renderer_grid,\n",
        "            loss_history_color,\n",
        "            loss_history_sil,\n",
        "        )\n",
        "        fig.savefig(f\"intermediate_{iteration}.png\")\n",
        "        plt.close(fig)\n",
        "\n",
        "print(\"‚úÖ Training complete\")# ===========================\n",
        "# 8) TRAIN LOOP\n",
        "# ===========================\n",
        "optimizer = torch.optim.Adam(neural_radiance_field.parameters(), lr=LR)\n",
        "batch_size = 6\n",
        "n_iter = N_ITERS\n",
        "\n",
        "loss_history_color, loss_history_sil = [], []\n",
        "\n",
        "for iteration in range(n_iter):\n",
        "    if iteration == round(n_iter * 0.75):\n",
        "        print(\"Decreasing LR 10-fold ...\")\n",
        "        optimizer = torch.optim.Adam(neural_radiance_field.parameters(), lr=LR * 0.1)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    batch_idx = torch.randperm(len(target_cameras))[:batch_size]\n",
        "\n",
        "    batch_cameras = FoVPerspectiveCameras(\n",
        "        R=target_cameras.R[batch_idx],\n",
        "        T=target_cameras.T[batch_idx],\n",
        "        znear=target_cameras.znear[batch_idx],\n",
        "        zfar=target_cameras.zfar[batch_idx],\n",
        "        aspect_ratio=target_cameras.aspect_ratio[batch_idx],\n",
        "        fov=target_cameras.fov[batch_idx],\n",
        "        device=device,\n",
        "    )\n",
        "\n",
        "    rendered_images_silhouettes, sampled_rays = renderer_mc(\n",
        "        cameras=batch_cameras,\n",
        "        volumetric_function=neural_radiance_field,\n",
        "    )\n",
        "    rendered_images, rendered_silhouettes = rendered_images_silhouettes.split([3, 1], dim=-1)\n",
        "\n",
        "    silhouettes_at_rays = sample_images_at_mc_locs(\n",
        "        target_silhouettes[batch_idx, ..., None], sampled_rays.xys\n",
        "    )\n",
        "    colors_at_rays = sample_images_at_mc_locs(\n",
        "        target_images[batch_idx], sampled_rays.xys\n",
        "    )\n",
        "\n",
        "    sil_err = huber(rendered_silhouettes, silhouettes_at_rays).abs().mean()\n",
        "    color_err = huber(rendered_images, colors_at_rays).abs().mean()\n",
        "    loss = color_err + sil_err\n",
        "\n",
        "    loss_history_color.append(float(color_err.detach()))\n",
        "    loss_history_sil.append(float(sil_err.detach()))\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if iteration % 100 == 0:\n",
        "        print(f\"Iteration {iteration}/{n_iter} | loss={loss.item():.4f}\")\n",
        "        show_idx = torch.randperm(len(target_cameras))[:1]\n",
        "        fig = show_full_render(\n",
        "            neural_radiance_field,\n",
        "            FoVPerspectiveCameras(\n",
        "                R=target_cameras.R[show_idx],\n",
        "                T=target_cameras.T[show_idx],\n",
        "                znear=target_cameras.znear[show_idx],\n",
        "                zfar=target_cameras.zfar[show_idx],\n",
        "                aspect_ratio=target_cameras.aspect_ratio[show_idx],\n",
        "                fov=target_cameras.fov[show_idx],\n",
        "                device=device,\n",
        "            ),\n",
        "            target_images[show_idx][0],\n",
        "            target_silhouettes[show_idx][0],\n",
        "            renderer_grid,\n",
        "            loss_history_color,\n",
        "            loss_history_sil,\n",
        "        )\n",
        "        fig.savefig(f\"intermediate_{iteration}.png\")\n",
        "        plt.close(fig)\n",
        "\n",
        "print(\"‚úÖ Training complete\")"
      ],
      "metadata": {
        "id": "6zs7wnlAWtsm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1812413-7d98-4a0e-87ed-b6df176a2271"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0/2000 | loss=0.3121\n",
            "Iteration 100/2000 | loss=0.1431\n",
            "Iteration 200/2000 | loss=0.0586\n",
            "Iteration 300/2000 | loss=0.0351\n",
            "Iteration 400/2000 | loss=0.0310\n",
            "Iteration 500/2000 | loss=0.0293\n",
            "Iteration 600/2000 | loss=0.0255\n",
            "Iteration 700/2000 | loss=0.0263\n",
            "Iteration 800/2000 | loss=0.0205\n",
            "Iteration 900/2000 | loss=0.0236\n",
            "Iteration 1000/2000 | loss=0.0171\n",
            "Iteration 1100/2000 | loss=0.0183\n",
            "Iteration 1200/2000 | loss=0.0142\n",
            "Iteration 1300/2000 | loss=0.0187\n",
            "Iteration 1400/2000 | loss=0.0160\n",
            "Decreasing LR 10-fold ...\n",
            "Iteration 1500/2000 | loss=0.0159\n",
            "Iteration 1600/2000 | loss=0.0199\n",
            "Iteration 1700/2000 | loss=0.0164\n",
            "Iteration 1800/2000 | loss=0.0125\n",
            "Iteration 1900/2000 | loss=0.0152\n",
            "‚úÖ Training complete\n",
            "Iteration 0/2000 | loss=0.0165\n",
            "Iteration 100/2000 | loss=0.0167\n",
            "Iteration 200/2000 | loss=0.0186\n",
            "Iteration 300/2000 | loss=0.0147\n",
            "Iteration 400/2000 | loss=0.0166\n",
            "Iteration 500/2000 | loss=0.0138\n",
            "Iteration 600/2000 | loss=0.0164\n",
            "Iteration 700/2000 | loss=0.0133\n",
            "Iteration 800/2000 | loss=0.0126\n",
            "Iteration 900/2000 | loss=0.0146\n",
            "Iteration 1000/2000 | loss=0.0180\n",
            "Iteration 1100/2000 | loss=0.0144\n",
            "Iteration 1200/2000 | loss=0.0122\n",
            "Iteration 1300/2000 | loss=0.0123\n",
            "Iteration 1400/2000 | loss=0.0101\n",
            "Decreasing LR 10-fold ...\n",
            "Iteration 1500/2000 | loss=0.0106\n",
            "Iteration 1600/2000 | loss=0.0104\n",
            "Iteration 1700/2000 | loss=0.0117\n",
            "Iteration 1800/2000 | loss=0.0141\n",
            "Iteration 1900/2000 | loss=0.0105\n",
            "‚úÖ Training complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# 9) SAVE MODEL & CONFIG (VERSIONED)\n",
        "# ===========================\n",
        "import os, json, time, torch\n",
        "\n",
        "SAVE_DIR = \"/content/drive/MyDrive/nerf\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "# Create timestamped filenames so nothing is overwritten\n",
        "timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "MODEL_PATH_NEW  = f\"{SAVE_DIR}/nerf_baseline_{timestamp}.pt\"\n",
        "CONFIG_PATH_NEW = f\"{SAVE_DIR}/config_baseline_{timestamp}.json\"\n",
        "\n",
        "# --- save weights ---\n",
        "torch.save(neural_radiance_field.state_dict(), MODEL_PATH_NEW)\n",
        "print(f\"üíæ Saved BASELINE model to: {MODEL_PATH_NEW}\")\n",
        "\n",
        "# --- save config (training + rendering intrinsics) ---\n",
        "cfg = {\n",
        "    \"fov\": float(target_cameras.fov[0]),\n",
        "    \"aspect_ratio\": float(target_cameras.aspect_ratio[0]),\n",
        "    \"znear\": float(target_cameras.znear[0]),\n",
        "    \"zfar\": float(target_cameras.zfar[0]),\n",
        "    \"render_size\": int(target_images.shape[1] * RENDER_SCALE),\n",
        "    \"volume_extent_world\": float(VOLUME_EXTENT_WORLD),\n",
        "\n",
        "    # Also store training hyperparams (optional but useful)\n",
        "    \"training_iters\": N_ITERS,\n",
        "    \"learning_rate\": LR,\n",
        "    \"mc_rays_per_image\": MC_RAYS_PER_IMAGE,\n",
        "    \"pts_per_ray\": PTS_PER_RAY,\n",
        "}\n",
        "\n",
        "with open(CONFIG_PATH_NEW, \"w\") as f:\n",
        "    json.dump(cfg, f, indent=2)\n",
        "\n",
        "print(f\"üíæ Saved config to: {CONFIG_PATH_NEW}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQC_QctYSzym",
        "outputId": "a5003fdd-6c99-4bea-e0af-623d93473771"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üíæ Saved BASELINE model to: /content/drive/MyDrive/nerf/nerf_baseline_20251118_093819.pt\n",
            "üíæ Saved config to: /content/drive/MyDrive/nerf/config_baseline_20251118_093819.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# 10) LOAD MODEL & RENDERER FOR VIEWER\n",
        "# ===========================\n",
        "from pytorch3d.renderer import (\n",
        "    FoVPerspectiveCameras,\n",
        "    NDCMultinomialRaysampler,\n",
        "    EmissionAbsorptionRaymarcher,\n",
        "    ImplicitRenderer,\n",
        ")\n",
        "BASELINE_MODEL_PATH = \"/content/drive/MyDrive/nerf/nerf_baseline_20251118_093819.pt\"\n",
        "BASELINE_CONFIG_PATH = \"/content/drive/MyDrive/nerf/config_baseline_20251118_093819.json\"\n",
        "\n",
        "# Load model fresh (useful if you restart)\n",
        "model = NeuralRadianceField().to(device)\n",
        "state = torch.load(BASELINE_MODEL_PATH, map_location=device)\n",
        "model.load_state_dict(state)\n",
        "model.eval()\n",
        "print(\"‚úÖ Model loaded:\", MODEL_PATH)\n",
        "\n",
        "# Load config\n",
        "with open(BASELINE_CONFIG_PATH) as f:\n",
        "    cfg = json.load(f)\n",
        "print(\"‚úÖ Config loaded\")\n",
        "\n",
        "# Viewer renderer\n",
        "raysampler_grid = NDCMultinomialRaysampler(\n",
        "    image_height=int(cfg[\"render_size\"]),\n",
        "    image_width =int(cfg[\"render_size\"]),\n",
        "    n_pts_per_ray=PTS_PER_RAY,\n",
        "    min_depth=0.1,\n",
        "    max_depth=float(cfg[\"volume_extent_world\"]),\n",
        ")\n",
        "renderer_grid = ImplicitRenderer(\n",
        "    raysampler=raysampler_grid,\n",
        "    raymarcher=EmissionAbsorptionRaymarcher(),\n",
        ").to(device)\n",
        "\n",
        "base_cameras = FoVPerspectiveCameras(\n",
        "    device=device,\n",
        "    znear=torch.tensor([cfg[\"znear\"]], device=device),\n",
        "    zfar=torch.tensor([cfg[\"zfar\"]], device=device),\n",
        "    aspect_ratio=torch.tensor([cfg[\"aspect_ratio\"]], device=device),\n",
        "    fov=torch.tensor([cfg[\"fov\"]], device=device),\n",
        ")\n",
        "print(\"‚úÖ Viewer renderer ready\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBE23t6UTWw7",
        "outputId": "ed61b08b-b3d3-4e77-c855-26ad5fb05e64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Model loaded: /content/drive/MyDrive/nerf/nerf_trained.pt\n",
            "‚úÖ Config loaded\n",
            "‚úÖ Viewer renderer ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# 11) GPU CACHE + INTERPOLATION + BLUR\n",
        "# ===========================\n",
        "from typing import Dict, Tuple\n",
        "\n",
        "CACHE_PATH = f\"{DRIVE_EXPORT}/views_AZ{AZ_STEP}_EL{EL_STEP}.npz\"\n",
        "\n",
        "_cache_tensor = None\n",
        "_key_to_idx: Dict[Tuple[int, int], int] = {}\n",
        "_H = _W = None\n",
        "\n",
        "def to_uint8_np_from_torch(img_t: torch.Tensor) -> np.ndarray:\n",
        "    return (img_t.clamp(0,1).detach().cpu().numpy() * 255).astype(np.uint8)\n",
        "\n",
        "@torch.no_grad()\n",
        "def render_full_gpu(elev_deg: float, azim_deg: float, dist: float = 2.7) -> torch.Tensor:\n",
        "    R, T = look_at_view_transform(dist=dist, elev=elev_deg, azim=azim_deg, device=device)\n",
        "    cam = FoVPerspectiveCameras(\n",
        "        R=R, T=T,\n",
        "        znear=base_cameras.znear, zfar=base_cameras.zfar,\n",
        "        aspect_ratio=base_cameras.aspect_ratio, fov=base_cameras.fov,\n",
        "        device=device,\n",
        "    )\n",
        "    img_sil, _ = renderer_grid(cameras=cam, volumetric_function=model.batched_forward)\n",
        "    return img_sil[..., :3].squeeze(0)\n",
        "\n",
        "@torch.no_grad()\n",
        "def precompute_cache_gpu():\n",
        "    global _cache_tensor, _key_to_idx, _H, _W\n",
        "    azs = np.arange(0, 361, AZ_STEP)\n",
        "    els = np.arange(-30, 31, EL_STEP)\n",
        "    imgs = []\n",
        "    _key_to_idx.clear()\n",
        "    idx = 0\n",
        "    for el in els:\n",
        "        for az in azs:\n",
        "            k = (int(az) % 360, int(np.clip(el, -30, 30)))\n",
        "            if k not in _key_to_idx:\n",
        "                img = render_full_gpu(el, az)\n",
        "                imgs.append(img)\n",
        "                _key_to_idx[k] = idx\n",
        "                idx += 1\n",
        "    _cache_tensor = torch.stack(imgs, dim=0)\n",
        "    _H, _W = _cache_tensor.shape[1:3]\n",
        "    print(f\"‚ö° Cached {len(_key_to_idx)} views on {device} (AZ {AZ_STEP}¬∞, EL {EL_STEP}¬∞).\")\n",
        "\n",
        "def save_cache_npz_gpu(path: str):\n",
        "    if _cache_tensor is None:\n",
        "        return\n",
        "    imgs_u8 = (_cache_tensor.clamp(0,1).mul(255).byte().cpu().numpy())\n",
        "    keys = np.array(list(_key_to_idx.keys()), dtype=object)\n",
        "    order = np.array([_key_to_idx[k] for k in _key_to_idx], dtype=np.int32)\n",
        "    sort_idx = np.argsort(order)\n",
        "    np.savez_compressed(path, keys=keys[sort_idx], imgs=imgs_u8[sort_idx])\n",
        "    print(f\"üíæ Saved GPU cache ‚Üí {path} ({len(keys)} views)\")\n",
        "\n",
        "def load_cache_npz_gpu(path: str) -> bool:\n",
        "    global _cache_tensor, _key_to_idx, _H, _W\n",
        "    if not os.path.exists(path):\n",
        "        return False\n",
        "    data = np.load(path, allow_pickle=True)\n",
        "    keys = list(data[\"keys\"])\n",
        "    imgs = data[\"imgs\"]\n",
        "    _key_to_idx.clear()\n",
        "    for i, k in enumerate(keys):\n",
        "        _key_to_idx[tuple(k)] = i\n",
        "    _cache_tensor = torch.from_numpy(imgs.astype(np.float32) / 255.0).to(device)\n",
        "    _H, _W = _cache_tensor.shape[1:3]\n",
        "    print(f\"üì• Loaded GPU cache from {path} ({len(_key_to_idx)} views).\")\n",
        "    return True\n",
        "\n",
        "def _get_four_indices(az, el):\n",
        "    az0 = int(np.floor(az / AZ_STEP) * AZ_STEP) % 360\n",
        "    az1 = (az0 + AZ_STEP) % 360\n",
        "    el0 = int(np.clip(np.floor((el + 30) / EL_STEP) * EL_STEP - 30, -30, 30))\n",
        "    el1 = int(np.clip(el0 + EL_STEP, -30, 30))\n",
        "    return (az0, el0), (az1, el0), (az0, el1), (az1, el1)\n",
        "\n",
        "@torch.no_grad()\n",
        "def bilinear_preview_gpu(az, el) -> torch.Tensor:\n",
        "    (az0, el0), (az1, el0b), (az0b, el1), (az1b, el1b) = _get_four_indices(az, el)\n",
        "    i00 = _key_to_idx[(az0, el0)]\n",
        "    i10 = _key_to_idx[(az1, el0b)]\n",
        "    i01 = _key_to_idx[(az0b, el1)]\n",
        "    i11 = _key_to_idx[(az1b, el1b)]\n",
        "    I00 = _cache_tensor[i00]\n",
        "    I10 = _cache_tensor[i10]\n",
        "    I01 = _cache_tensor[i01]\n",
        "    I11 = _cache_tensor[i11]\n",
        "    t = torch.tensor(((az - az0) % 360) / AZ_STEP, device=device).float()\n",
        "    u = torch.tensor((el - el0) / max(EL_STEP, 1e-6), device=device).float()\n",
        "    top = (1 - t) * I00 + t * I10\n",
        "    bot = (1 - t) * I01 + t * I11\n",
        "    return (1 - u) * top + u * bot\n",
        "\n",
        "def gaussian_kernel1d(radius: int, sigma: float, device):\n",
        "    x = torch.arange(-radius, radius+1, device=device)\n",
        "    w = torch.exp(-(x**2)/(2*sigma*sigma))\n",
        "    w = w / w.sum()\n",
        "    return w\n",
        "\n",
        "@torch.no_grad()\n",
        "def blur_preview_gpu(img: torch.Tensor, radius: int = 2, sigma: float = 1.5) -> torch.Tensor:\n",
        "    k1d = gaussian_kernel1d(radius, sigma, device=img.device)\n",
        "    x = img.permute(2,0,1).unsqueeze(0)\n",
        "    kh = k1d.view(1,1,1,-1)\n",
        "    x = F.conv2d(x, kh.expand(3,1,1,kh.shape[-1]), padding=(0, radius), groups=3)\n",
        "    kv = k1d.view(1,1,-1,1)\n",
        "    x = F.conv2d(x, kv.expand(3,1,kv.shape[-2],1), padding=(radius, 0), groups=3)\n",
        "    return x.squeeze(0).permute(1,2,0)"
      ],
      "metadata": {
        "id": "mUyvTr6yWyBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- Helpers to pick the latest cache automatically ----\n",
        "import io, hashlib, glob, os, re, time\n",
        "\n",
        "def model_md5(model) -> str:\n",
        "    buf = io.BytesIO()\n",
        "    torch.save(model.state_dict(), buf)\n",
        "    return hashlib.md5(buf.getvalue()).hexdigest()[:8]\n",
        "\n",
        "def find_latest_cache(cache_dir: str, az_step: int, el_step: int, preferred_hash: str | None):\n",
        "    \"\"\"\n",
        "    Return (path, reason) where `path` is the best cache to use:\n",
        "      1) exact match for AZ/EL and preferred_hash, newest mtime if multiple\n",
        "      2) else newest file that matches AZ/EL regardless of hash\n",
        "      3) else None\n",
        "    Expected filename pattern: views_AZ{az}_EL{el}_{hash}.npz\n",
        "    \"\"\"\n",
        "    pattern = os.path.join(cache_dir, f\"views_AZ{az_step}_EL{el_step}_*.npz\")\n",
        "    candidates = glob.glob(pattern)\n",
        "    if not candidates:\n",
        "        return None, \"no cache files found\"\n",
        "\n",
        "    # extract (mtime, path, hash)\n",
        "    rx = re.compile(rf\"views_AZ{az_step}_EL{el_step}_(?P<h>[0-9a-fA-F]+)\\.npz$\")\n",
        "    parsed = []\n",
        "    for p in candidates:\n",
        "        m = rx.search(os.path.basename(p))\n",
        "        h = m.group(\"h\") if m else None\n",
        "        parsed.append((os.path.getmtime(p), p, h))\n",
        "\n",
        "    # 1) prefer exact hash match if available\n",
        "    if preferred_hash:\n",
        "        exact = [t for t in parsed if t[2] == preferred_hash]\n",
        "        if exact:\n",
        "            exact.sort(key=lambda t: t[0], reverse=True)\n",
        "            return exact[0][1], \"match model hash\"\n",
        "\n",
        "    # 2) otherwise newest by mtime\n",
        "    parsed.sort(key=lambda t: t[0], reverse=True)\n",
        "    return parsed[0][1], \"newest by mtime\""
      ],
      "metadata": {
        "id": "Nq1ulAR4WNYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# 12) BUILD/LOAD CACHE + CALLBACKS (auto-pick latest)\n",
        "# ===========================\n",
        "FORCE_REBUILD_CACHE = False  # set True to force regeneration\n",
        "\n",
        "# derive preferred cache name for *this* model\n",
        "PREFERRED_HASH = model_md5(model)\n",
        "CACHE_DIR = DRIVE_EXPORT\n",
        "\n",
        "# Optionally print which cache we intend to use\n",
        "print(f\"‚ÑπÔ∏è Preferred cache hash for this model: {PREFERRED_HASH}\")\n",
        "\n",
        "def build_and_save_cache():\n",
        "    global _cache_tensor, _key_to_idx, _H, _W\n",
        "    _cache_tensor = None\n",
        "    _key_to_idx.clear()\n",
        "    torch.cuda.empty_cache()\n",
        "    precompute_cache_gpu()\n",
        "    # name includes steps + model hash so caches don‚Äôt get mixed up\n",
        "    out_path = os.path.join(CACHE_DIR, f\"views_AZ{AZ_STEP}_EL{EL_STEP}_{PREFERRED_HASH}.npz\")\n",
        "    save_cache_npz_gpu(out_path)\n",
        "    return out_path\n",
        "\n",
        "if FORCE_REBUILD_CACHE:\n",
        "    print(\"üîÅ FORCE_REBUILD_CACHE=True ‚Üí rebuilding cache now‚Ä¶\")\n",
        "    CACHE_PATH = build_and_save_cache()\n",
        "else:\n",
        "    # try to find the best existing cache\n",
        "    CACHE_PATH, reason = find_latest_cache(CACHE_DIR, AZ_STEP, EL_STEP, preferred_hash=PREFERRED_HASH)\n",
        "    if CACHE_PATH is not None and load_cache_npz_gpu(CACHE_PATH):\n",
        "        print(f\"‚úÖ Loaded cache: {CACHE_PATH} ({reason})\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è No suitable cache found ‚Üí building a new one‚Ä¶\")\n",
        "        CACHE_PATH = build_and_save_cache()\n",
        "\n",
        "@torch.no_grad()\n",
        "def on_change(azim, elev, mode, current_img):\n",
        "    img = bilinear_preview_gpu(azim, elev)\n",
        "    if mode == \"Blur preview\" and img is not None:\n",
        "        img = blur_preview_gpu(img, radius=2, sigma=1.5)\n",
        "    return to_uint8_np_from_torch(img)\n",
        "\n",
        "@torch.no_grad()\n",
        "def on_release(azim, elev):\n",
        "    img = render_full_gpu(elev, azim)\n",
        "    return to_uint8_np_from_torch(img)\n",
        "\n",
        "START_IMG_T = render_full_gpu(START_EL, START_AZ)\n",
        "START_IMG = to_uint8_np_from_torch(START_IMG_T)\n",
        "H, W = START_IMG.shape[:2]\n",
        "print(\"‚úÖ Viewer callbacks ready\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OM369M3MgLW6",
        "outputId": "bace4571-de12-4a91-8fd7-11139a7d6872"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ÑπÔ∏è Preferred cache hash for this model: 1b661045\n",
            "üì• Loaded GPU cache from /content/drive/MyDrive/nerf/views_AZ15_EL10_65f687b2.npz (168 views).\n",
            "‚úÖ Loaded cache: /content/drive/MyDrive/nerf/views_AZ15_EL10_65f687b2.npz (newest by mtime)\n",
            "‚úÖ Viewer callbacks ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# 13) GRADIO UI (MOUSE DRAG ROTATION)\n",
        "# ===========================\n",
        "import gradio as gr\n",
        "\n",
        "with gr.Blocks(title=\"NeRF Viewer\") as demo:\n",
        "    gr.Markdown(\"## üêÑ NeRF Interactive Viewer\\nDrag on the image to rotate. Release to render full quality.\")\n",
        "    with gr.Row():\n",
        "        image_out = gr.Image(\n",
        "            value=START_IMG, label=\"Render\",\n",
        "            type=\"numpy\", height=H, width=W, interactive=True\n",
        "        )\n",
        "        with gr.Column(scale=0):\n",
        "            az = gr.Slider(0, 360, value=START_AZ, step=1, label=\"Azimuth (¬∞)\")\n",
        "            el = gr.Slider(-30, 30, value=START_EL, step=1, label=\"Elevation (¬∞)\")\n",
        "            quality = gr.Radio(\n",
        "                choices=[\"Bilinear preview\", \"Blur preview\"],\n",
        "                value=\"Bilinear preview\",\n",
        "                label=\"Drag preview\"\n",
        "            )\n",
        "\n",
        "    # Client-side JS: drag the image to update sliders (which triggers previews)\n",
        "    drag_bind = gr.HTML(\"\"\"\n",
        "    <script>\n",
        "    (function(){\n",
        "      const sleep = (ms) => new Promise(r => setTimeout(r, ms));\n",
        "      async function bind() {\n",
        "        for (let i=0;i<50;i++){\n",
        "          const app = window.gradioApp?.();\n",
        "          if (app) break;\n",
        "          await sleep(100);\n",
        "        }\n",
        "        const app = window.gradioApp?.();\n",
        "        if (!app) return;\n",
        "        const img = app.querySelector('div.svelte-1ipelgc img, .image-container img');\n",
        "        const az = app.querySelector('input[type=\"range\"][min=\"0\"][max=\"360\"]');\n",
        "        const el = app.querySelector('input[type=\"range\"][min=\"-30\"][max=\"30\"]');\n",
        "        if (!img || !az || !el) return;\n",
        "\n",
        "        let dragging = false, lastX = 0, lastY = 0;\n",
        "        const clamp = (v,min,max)=>Math.max(min,Math.min(max,v));\n",
        "        const step = (v,s)=>Math.round(v/s)*s;\n",
        "\n",
        "        img.addEventListener('mousedown', (e)=>{\n",
        "          dragging = true; lastX = e.clientX; lastY = e.clientY; e.preventDefault();\n",
        "        });\n",
        "        window.addEventListener('mouseup', ()=>{ dragging=false; });\n",
        "        window.addEventListener('mousemove', (e)=>{\n",
        "          if(!dragging) return;\n",
        "          const dx = e.clientX - lastX;\n",
        "          const dy = e.clientY - lastY;\n",
        "          lastX = e.clientX; lastY = e.clientY;\n",
        "\n",
        "          const AZ_SENS = 0.5;\n",
        "          const EL_SENS = 0.3;\n",
        "\n",
        "          let azVal = (parseFloat(az.value) + dx * AZ_SENS) % 360;\n",
        "          if(azVal < 0) azVal += 360;\n",
        "          let elVal = clamp(parseFloat(el.value) - dy * EL_SENS, -30, 30);\n",
        "\n",
        "          az.value = String(step(azVal,1));\n",
        "          el.value = String(step(elVal,1));\n",
        "\n",
        "          az.dispatchEvent(new Event('input', {bubbles:true}));\n",
        "          el.dispatchEvent(new Event('input', {bubbles:true}));\n",
        "        });\n",
        "        window.addEventListener('mouseup', ()=>{\n",
        "          az.dispatchEvent(new Event('change', {bubbles:true}));\n",
        "          el.dispatchEvent(new Event('change', {bubbles:true}));\n",
        "        });\n",
        "      }\n",
        "      bind();\n",
        "    })();\n",
        "    </script>\n",
        "    \"\"\")\n",
        "\n",
        "    az.change(on_change, inputs=[az, el, quality, image_out], outputs=image_out, queue=False)\n",
        "    el.change(on_change, inputs=[az, el, quality, image_out], outputs=image_out, queue=False)\n",
        "    az.release(on_release, inputs=[az, el], outputs=image_out, queue=True)\n",
        "    el.release(on_release, inputs=[az, el], outputs=image_out, queue=True)\n",
        "\n",
        "demo.launch(inline=True, debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "id": "YEnHytC86Btd",
        "outputId": "f9c6ee5c-345b-4c98-cb35-22b076c167c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://615c9bc673a7b63f2e.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://615c9bc673a7b63f2e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://615c9bc673a7b63f2e.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import random\n",
        "\n",
        "def time_bilinear_preview(az=None, el=None):\n",
        "    if az is None: az = random.uniform(0, 360)\n",
        "    if el is None: el = random.uniform(-30, 30)\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    t0 = time.perf_counter()\n",
        "\n",
        "    img = bilinear_preview_gpu(az, el)\n",
        "    img_np = to_uint8_np_from_torch(img)\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    return (time.perf_counter() - t0) * 1000\n",
        "\n",
        "\n",
        "def time_blur_preview(az=None, el=None):\n",
        "    if az is None: az = random.uniform(0, 360)\n",
        "    if el is None: el = random.uniform(-30, 30)\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    t0 = time.perf_counter()\n",
        "\n",
        "    img = bilinear_preview_gpu(az, el)            # base preview\n",
        "    img = blur_preview_gpu(img, radius=2, sigma=1.5)  # additional blur\n",
        "    img_np = to_uint8_np_from_torch(img)\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    return (time.perf_counter() - t0) * 1000\n",
        "\n",
        "\n",
        "def time_full_render(az=None, el=None):\n",
        "    if az is None: az = random.uniform(0, 360)\n",
        "    if el is None: el = random.uniform(-30, 30)\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    t0 = time.perf_counter()\n",
        "\n",
        "    img = render_full_gpu(el, az)\n",
        "    img_np = to_uint8_np_from_torch(img)\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    return (time.perf_counter() - t0) * 1000"
      ],
      "metadata": {
        "id": "YPRcKMkeM9a3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# üìä MODEL PERFORMANCE SUMMARY (SINGLE MODEL)\n",
        "# Run this AFTER loading whichever model you want to test\n",
        "# (baseline NeuralRadianceField OR HashNeuralRadianceField)\n",
        "# ============================================\n",
        "import torch, time, random, gc\n",
        "\n",
        "def count_parameters(m):\n",
        "    return sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
        "\n",
        "def time_full_render_single(n=10, az=None, el=None):\n",
        "    \"\"\"\n",
        "    Uses global `model` and `render_full_gpu(elev, azim)`.\n",
        "    Measures latency of full NeRF render.\n",
        "    \"\"\"\n",
        "    times = []\n",
        "    torch.cuda.empty_cache()\n",
        "    for _ in range(n):\n",
        "        az_ = random.uniform(0, 360) if az is None else az\n",
        "        el_ = random.uniform(-30, 30) if el is None else el\n",
        "\n",
        "        torch.cuda.synchronize()\n",
        "        t0 = time.perf_counter()\n",
        "\n",
        "        img = render_full_gpu(el_, az_)  # uses global model.batched_forward\n",
        "        _ = img.detach().cpu().numpy()\n",
        "\n",
        "        torch.cuda.synchronize()\n",
        "        times.append((time.perf_counter() - t0) * 1000)\n",
        "\n",
        "    return {\n",
        "        \"avg_ms\": sum(times)/len(times),\n",
        "        \"min_ms\": min(times),\n",
        "        \"max_ms\": max(times),\n",
        "    }\n",
        "\n",
        "def time_bilinear_single(n=50):\n",
        "    \"\"\"\n",
        "    Uses global bilinear_preview_gpu, which relies on the current cache.\n",
        "    Make sure cache matches the current model.\n",
        "    \"\"\"\n",
        "    times = []\n",
        "    for _ in range(n):\n",
        "        az = random.uniform(0, 360)\n",
        "        el = random.uniform(-30, 30)\n",
        "\n",
        "        torch.cuda.synchronize()\n",
        "        t0 = time.perf_counter()\n",
        "\n",
        "        img = bilinear_preview_gpu(az, el)\n",
        "        _ = img.detach().cpu().numpy()\n",
        "\n",
        "        torch.cuda.synchronize()\n",
        "        times.append((time.perf_counter() - t0) * 1000)\n",
        "\n",
        "    return sum(times)/len(times)\n",
        "\n",
        "def time_blur_single(n=50):\n",
        "    times = []\n",
        "    for _ in range(n):\n",
        "        az = random.uniform(0, 360)\n",
        "        el = random.uniform(-30, 30)\n",
        "\n",
        "        torch.cuda.synchronize()\n",
        "        t0 = time.perf_counter()\n",
        "\n",
        "        img = bilinear_preview_gpu(az, el)\n",
        "        img = blur_preview_gpu(img, radius=2, sigma=1.5)\n",
        "        _ = img.detach().cpu().numpy()\n",
        "\n",
        "        torch.cuda.synchronize()\n",
        "        times.append((time.perf_counter() - t0) * 1000)\n",
        "\n",
        "    return sum(times)/len(times)\n",
        "\n",
        "# -----------------------------\n",
        "# Run metrics for *current* model\n",
        "# -----------------------------\n",
        "model_name = model.__class__.__name__\n",
        "print(f\"\\n==============================\")\n",
        "print(f\"üìå Performance Summary ‚Äì {model_name}\")\n",
        "print(f\"==============================\")\n",
        "\n",
        "print(f\"Total parameters: {count_parameters(model):,}\")\n",
        "\n",
        "# Preview latencies (cache-based)\n",
        "bilinear_ms = time_bilinear_single()\n",
        "blur_ms     = time_blur_single()\n",
        "\n",
        "print(f\"Bilinear preview avg latency: {bilinear_ms:.3f} ms\")\n",
        "print(f\"Blur preview avg latency:     {blur_ms:.3f} ms\")\n",
        "\n",
        "# Full render speed\n",
        "full_stats = time_full_render_single()\n",
        "print(f\"Full render time (ms): avg={full_stats['avg_ms']:.1f}, \"\n",
        "      f\"min={full_stats['min_ms']:.1f}, max={full_stats['max_ms']:.1f}\")\n",
        "\n",
        "# Memory usage\n",
        "torch.cuda.synchronize()\n",
        "alloc = torch.cuda.memory_allocated() / (1024**2)\n",
        "reserved = torch.cuda.memory_reserved() / (1024**2)\n",
        "print(f\"GPU memory allocated: {alloc:.1f} MB\")\n",
        "print(f\"GPU memory reserved:  {reserved:.1f} MB\")\n",
        "\n",
        "print(\"==============================\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74xZTibWtLed",
        "outputId": "410036cf-7cd7-4bc5-8a86-b13d3be02027"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "üìå Performance Summary ‚Äì NeuralRadianceField\n",
            "==============================\n",
            "Total parameters: 317,188\n",
            "Bilinear preview avg latency: 0.782 ms\n",
            "Blur preview avg latency:     0.979 ms\n",
            "Full render time (ms): avg=1680.9, min=1647.9, max=1716.0\n",
            "GPU memory allocated: 161.7 MB\n",
            "GPU memory reserved:  3970.0 MB\n",
            "==============================\n",
            "\n"
          ]
        }
      ]
    }
  ]
}